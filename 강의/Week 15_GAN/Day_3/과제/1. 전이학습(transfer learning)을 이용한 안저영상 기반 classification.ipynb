{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"1. 전이학습(transfer learning)을 이용한 안저영상 기반 classification.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"_uuid":"34b6997bb115a11f47a7f54ce9d9052791b2e707","_cell_guid":"c67e806c-e0e6-415b-8cf0-ed92eba5ed37","id":"FGC_5u6TBkzT"},"source":["# InceptionV3를 사용해서 transfer learning 실행."]},{"cell_type":"markdown","metadata":{"id":"3r5B3kyNBkzZ"},"source":["## 1. dataset 및 라이브러리"]},{"cell_type":"code","metadata":{"_uuid":"c163e45042a69905855f7c04a65676e5aca4837b","_cell_guid":"e94de3e7-de1f-4c18-ad1f-c8b686127340","execution":{"iopub.status.busy":"2021-08-16T12:29:22.521687Z","iopub.execute_input":"2021-08-16T12:29:22.522052Z","iopub.status.idle":"2021-08-16T12:29:26.655331Z","shell.execute_reply.started":"2021-08-16T12:29:22.521993Z","shell.execute_reply":"2021-08-16T12:29:26.654292Z"},"trusted":true,"id":"C6SziUv9Bkzb","outputId":"ef606aa9-1d97-44d2-fb95-73f6c871bc49"},"source":["# Kaggle notebook 사용\n","!mkdir ~/.keras\n","!mkdir ~/.keras/models\n","# !cp ../input/keras-pretrained-models/*notop* ~/.keras/models/\n","# !cp ../input/keras-pretrained-models/imagenet_class_index.json ~/.keras/models/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘/root/.keras’: File exists\n","mkdir: cannot create directory ‘/root/.keras/models’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_uuid":"725d378daf5f836d4885d67240fc7955f113309d","_cell_guid":"c3cc4285-bfa4-4612-ac5f-13d10678c09a","execution":{"iopub.status.busy":"2021-08-16T12:29:45.122622Z","iopub.execute_input":"2021-08-16T12:29:45.122980Z","iopub.status.idle":"2021-08-16T12:29:45.665234Z","shell.execute_reply.started":"2021-08-16T12:29:45.122925Z","shell.execute_reply":"2021-08-16T12:29:45.664425Z"},"trusted":true,"id":"rVzzS8TdBkzf"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from skimage.io import imread\n","import os\n","from glob import glob\n","# not needed in Kaggle, but required in Jupyter\n","# %matplotlib inline "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eh1R92EjBkzh"},"source":["# 2. EDA"]},{"cell_type":"code","metadata":{"_uuid":"346da81db6ee7a34af8da8af245b42e681f2ba48","_cell_guid":"c4b38df6-ffa1-4847-b605-511e72b68231","execution":{"iopub.status.busy":"2021-08-16T12:30:22.426002Z","iopub.execute_input":"2021-08-16T12:30:22.426353Z","iopub.status.idle":"2021-08-16T12:30:52.449464Z","shell.execute_reply.started":"2021-08-16T12:30:22.426295Z","shell.execute_reply":"2021-08-16T12:30:52.447997Z"},"trusted":true,"id":"skz8PVk2Bkzh","outputId":"e1122960-9752-4104-f4e6-51c9558ba882"},"source":["base_image_dir = os.path.join('..', 'input', 'diabetic-retinopathy-detection')\n","retina_df = pd.read_csv(os.path.join(base_image_dir, 'trainLabels.csv.zip'))\n","retina_df['PatientId'] = retina_df['image'].map(lambda x: x.split('_')[0])\n","retina_df['path'] = retina_df['image'].map(lambda x: os.path.join(base_image_dir,\n","                                                         '{}.jpeg'.format(x)))\n","retina_df['exists'] = retina_df['path'].map(os.path.exists)\n","print(retina_df['exists'].sum(), 'images found of', retina_df.shape[0], 'total')\n","retina_df['eye'] = retina_df['image'].map(lambda x: 1 if x.split('_')[-1]=='left' else 0)\n","from keras.utils.np_utils import to_categorical\n","retina_df['level_cat'] = retina_df['level'].map(lambda x: to_categorical(x, 1+retina_df['level'].max()))\n","\n","retina_df.dropna(inplace = True)\n","retina_df = retina_df[retina_df['exists']]\n","retina_df.sample(3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 images found of 35126 total\n"],"name":"stdout"},{"output_type":"stream","text":["/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n","  from ._conv import register_converters as _register_converters\n","Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"error","ename":"ValueError","evalue":"a must be greater than 0","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-03c6e0e5b062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mretina_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mretina_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mretina_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mretina_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[1;32m   3439\u001b[0m                              \"provide positive value.\")\n\u001b[1;32m   3440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3441\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3442\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_copy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: a must be greater than 0"]}]},{"cell_type":"code","metadata":{"_uuid":"60a8111c4093ca6f69d27a4499442ba7dd750839","_cell_guid":"5c8bd288-8261-4cbe-a954-e62ac795cc3e","trusted":true,"id":"XSTJGxQUBkzi"},"source":["retina_df[['level', 'eye']].hist(figsize = (10, 5))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"4df45776bae0b8a1bf9d3eb4eaaebce6e24d726d","_cell_guid":"0ba697ed-85bb-4e9a-9765-4c367db078d1","id":"9IctPNsDBkzj"},"source":["## 3. Split Data into Training and Validation"]},{"cell_type":"code","metadata":{"_uuid":"a48b300ca4d37a6e8b39f82e3c172739635e4baa","_cell_guid":"1192c6b3-a940-4fa0-a498-d7e0d400a796","trusted":true,"id":"poZuaDEVBkzl"},"source":["from sklearn.model_selection import train_test_split\n","\n","\n","rr_df = retina_df[['PatientId', 'level']].drop_duplicates()\n","train_ids, valid_ids = train_test_split(rr_df['PatientId'], \n","                                   test_size = 0.25, \n","                                   random_state = 2018,\n","                                   stratify = rr_df['level'])\n","raw_train_df = retina_df[retina_df['PatientId'].isin(train_ids)]\n","valid_df = retina_df[retina_df['PatientId'].isin(valid_ids)]\n","print('train', raw_train_df.shape[0], 'validation', valid_df.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"26e566d6cec5bd41f9afe392f456ddf7ceb306ea","_cell_guid":"f8060459-da1e-4293-8f61-c7f99de1de9f","id":"2lkKiBvLBkzn"},"source":["## 4. Balance the distribution in the training set"]},{"cell_type":"code","metadata":{"_uuid":"ba7befa238b8c11f9672e3539ac58f3da6955bd9","_cell_guid":"7a130199-fbf6-4c60-95f5-0797b2f3eaf1","trusted":true,"id":"rUPc6BXVBkzn"},"source":["train_df = raw_train_df.groupby(['level', 'eye']).apply(lambda x: x.sample(75, replace = True)\n","                                                      ).reset_index(drop = True)\n","print('New Data Size:', train_df.shape[0], 'Old Size:', raw_train_df.shape[0])\n","train_df[['level', 'eye']].hist(figsize = (10, 5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"9529ab766763a9f122786464c24ab1ebe22c6006","_cell_guid":"9954bfda-29bd-4c4d-b526-0a972b3e43e2","trusted":true,"id":"QiAG5tv2Bkzn"},"source":["import tensorflow as tf\n","from keras import backend as K\n","from keras.applications.inception_v3 import preprocess_input\n","import numpy as np\n","\n","\n","IMG_SIZE = (512, 512) # slightly smaller than vgg16 normally expects\n","\n","\n","def tf_image_loader(out_size, \n","                      horizontal_flip = True, \n","                      vertical_flip = False, \n","                     random_brightness = True,\n","                     random_contrast = True,\n","                    random_saturation = True,\n","                    random_hue = True,\n","                      color_mode = 'rgb',\n","                       preproc_func = preprocess_input,\n","                       on_batch = False):\n","    \n","    def _func(X):\n","        with tf.name_scope('image_augmentation'):\n","            with tf.name_scope('input'):\n","                X = tf.image.decode_png(tf.read_file(X), channels = 3 if color_mode == 'rgb' else 0)\n","                X = tf.image.resize_images(X, out_size)\n","            with tf.name_scope('augmentation'):\n","                if horizontal_flip:\n","                    X = tf.image.random_flip_left_right(X)\n","                if vertical_flip:\n","                    X = tf.image.random_flip_up_down(X)\n","                if random_brightness:\n","                    X = tf.image.random_brightness(X, max_delta = 0.1)\n","                if random_saturation:\n","                    X = tf.image.random_saturation(X, lower = 0.75, upper = 1.5)\n","                if random_hue:\n","                    X = tf.image.random_hue(X, max_delta = 0.15)\n","                if random_contrast:\n","                    X = tf.image.random_contrast(X, lower = 0.75, upper = 1.5)\n","                return preproc_func(X)\n","    if on_batch: \n","        # we are meant to use it on a batch\n","        def _batch_func(X, y):\n","            return tf.map_fn(_func, X), y\n","        return _batch_func\n","    else:\n","        # we apply it to everything\n","        def _all_func(X, y):\n","            return _func(X), y         \n","        return _all_func\n","    \n","    \n","def tf_augmentor(out_size,\n","                intermediate_size = (640, 640),\n","                 intermediate_trans = 'crop',\n","                 batch_size = 16,\n","                   horizontal_flip = True, \n","                  vertical_flip = False, \n","                 random_brightness = True,\n","                 random_contrast = True,\n","                 random_saturation = True,\n","                    random_hue = True,\n","                  color_mode = 'rgb',\n","                   preproc_func = preprocess_input,\n","                   min_crop_percent = 0.001,\n","                   max_crop_percent = 0.005,\n","                   crop_probability = 0.5,\n","                   rotation_range = 10):\n","    \n","    load_ops = tf_image_loader(out_size = intermediate_size, \n","                               horizontal_flip=horizontal_flip, \n","                               vertical_flip=vertical_flip, \n","                               random_brightness = random_brightness,\n","                               random_contrast = random_contrast,\n","                               random_saturation = random_saturation,\n","                               random_hue = random_hue,\n","                               color_mode = color_mode,\n","                               preproc_func = preproc_func,\n","                               on_batch=False)\n","    \n","    \n","    def batch_ops(X, y):\n","        batch_size = tf.shape(X)[0]\n","        with tf.name_scope('transformation'):\n","            # code borrowed from https://becominghuman.ai/data-augmentation-on-gpu-in-tensorflow-13d14ecf2b19\n","            # The list of affine transformations that our image will go under.\n","            # Every element is Nx8 tensor, where N is a batch size.\n","            transforms = []\n","            identity = tf.constant([1, 0, 0, 0, 1, 0, 0, 0], dtype=tf.float32)\n","            if rotation_range > 0:\n","                angle_rad = rotation_range / 180 * np.pi\n","                angles = tf.random_uniform([batch_size], -angle_rad, angle_rad)\n","                transforms += [tf.contrib.image.angles_to_projective_transforms(angles, intermediate_size[0], intermediate_size[1])]\n","\n","            if crop_probability > 0:\n","                crop_pct = tf.random_uniform([batch_size], min_crop_percent, max_crop_percent)\n","                left = tf.random_uniform([batch_size], 0, intermediate_size[0] * (1.0 - crop_pct))\n","                top = tf.random_uniform([batch_size], 0, intermediate_size[1] * (1.0 - crop_pct))\n","                crop_transform = tf.stack([\n","                      crop_pct,\n","                      tf.zeros([batch_size]), top,\n","                      tf.zeros([batch_size]), crop_pct, left,\n","                      tf.zeros([batch_size]),\n","                      tf.zeros([batch_size])\n","                  ], 1)\n","                coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), crop_probability)\n","                transforms += [tf.where(coin, crop_transform, tf.tile(tf.expand_dims(identity, 0), [batch_size, 1]))]\n","            if len(transforms)>0:\n","                X = tf.contrib.image.transform(X,\n","                      tf.contrib.image.compose_transforms(*transforms),\n","                      interpolation='BILINEAR') # or 'NEAREST'\n","            if intermediate_trans=='scale':\n","                X = tf.image.resize_images(X, out_size)\n","            elif intermediate_trans=='crop':\n","                X = tf.image.resize_image_with_crop_or_pad(X, out_size[0], out_size[1])\n","            else:\n","                raise ValueError('Invalid Operation {}'.format(intermediate_trans))\n","            return X, y\n","        \n","        \n","    def _create_pipeline(in_ds):\n","        batch_ds = in_ds.map(load_ops, num_parallel_calls=4).batch(batch_size)\n","        return batch_ds.map(batch_ops)\n","    return _create_pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"07851e798db3d89ba13db7d4b56ab2b759221464","_cell_guid":"b5767f42-da63-4737-8f50-749c1a25aa84","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true,"id":"uh1BlOIgBkzp"},"source":["def flow_from_dataframe(idg, \n","                        in_df, \n","                        path_col,\n","                        y_col, \n","                        shuffle = True, \n","                        color_mode = 'rgb'):\n","    files_ds = tf.data.Dataset.from_tensor_slices((in_df[path_col].values, \n","                                                   np.stack(in_df[y_col].values,0)))\n","    in_len = in_df[path_col].values.shape[0]\n","    while True:\n","        if shuffle:\n","            files_ds = files_ds.shuffle(in_len) # shuffle the whole dataset\n","        \n","        next_batch = idg(files_ds).repeat().make_one_shot_iterator().get_next()\n","        for i in range(max(in_len//32,1)):\n","            # NOTE: if we loop here it is 'thread-safe-ish' if we loop on the outside it is completely unsafe\n","            yield K.get_session().run(next_batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"1848f5048a9e00668c3778a85deea97f980e4f1c","_cell_guid":"810bd229-fec9-43c4-b3bd-afd62e3e9552","trusted":true,"id":"Dz2Ps1bHBkzq"},"source":["batch_size = 48\n","core_idg = tf_augmentor(out_size = IMG_SIZE, \n","                        color_mode = 'rgb', \n","                        vertical_flip = True,\n","                        crop_probability=0.0, # crop doesn't work yet\n","                        batch_size = batch_size) \n","valid_idg = tf_augmentor(out_size = IMG_SIZE, color_mode = 'rgb', \n","                         crop_probability=0.0, \n","                         horizontal_flip = False, \n","                         vertical_flip = False, \n","                         random_brightness = False,\n","                         random_contrast = False,\n","                         random_saturation = False,\n","                         random_hue = False,\n","                         rotation_range = 0,\n","                        batch_size = batch_size)\n","\n","train_gen = flow_from_dataframe(core_idg, train_df, \n","                             path_col = 'path',\n","                            y_col = 'level_cat')\n","\n","valid_gen = flow_from_dataframe(valid_idg, valid_df, \n","                             path_col = 'path',\n","                            y_col = 'level_cat') # we can use much larger batches for evaluation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"2ad184b936d5cebae91a265a247d8e0e25920566","id":"xE5TemzkBkzr"},"source":["## 4-1. Validation Set 확인"]},{"cell_type":"code","metadata":{"_uuid":"6810407e25b887dd8b352f1e46fb3faceaa58ab7","trusted":true,"id":"wQHmAQjYBkzr"},"source":["t_x, t_y = next(valid_gen)\n","fig, m_axs = plt.subplots(2, 4, figsize = (16, 8))\n","for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n","    c_ax.imshow(np.clip(c_x*127+127, 0, 255).astype(np.uint8))\n","    c_ax.set_title('Severity {}'.format(np.argmax(c_y, -1)))\n","    c_ax.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"34ce892a19c9734511e2da1d0f2552b361dc826d","id":"6T8gpbEOBkzr"},"source":["## 4-2. Training Set 확인"]},{"cell_type":"code","metadata":{"_uuid":"8190b4ad60d49fa65af074dd138a19cb8787e983","scrolled":true,"_cell_guid":"2d62234f-aeb0-4eba-8a38-d713d819abf6","trusted":true,"id":"hSw9l839Bkzs"},"source":["t_x, t_y = next(train_gen)\n","fig, m_axs = plt.subplots(2, 4, figsize = (16, 8))\n","for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n","    c_ax.imshow(np.clip(c_x*127+127, 0, 255).astype(np.uint8))\n","    c_ax.set_title('Severity {}'.format(np.argmax(c_y, -1)))\n","    c_ax.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"55d665e1e8a8d83b9db005a66a965f8a90c62da1","_cell_guid":"da22790a-672c-474e-b118-9eef15b53160","id":"oR4C1I90Bkzs"},"source":["## 5. Attention Model"]},{"cell_type":"code","metadata":{"_uuid":"1f0dfaccda346d7bc4758e7329d61028d254a8d6","_cell_guid":"eeb36110-0cde-4450-a43c-b8f707adb235","trusted":true,"id":"szuLzsV2Bkzs"},"source":["from keras.applications.vgg16 import VGG16 as PTModel\n","from keras.applications.inception_resnet_v2 import InceptionResNetV2 as PTModel\n","from keras.applications.inception_v3 import InceptionV3 as PTModel\n","from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda\n","from keras.models import Model\n","\n","\n","in_lay = Input(t_x.shape[1:])\n","base_pretrained_model = PTModel(input_shape =  t_x.shape[1:], include_top = False, weights = 'imagenet')\n","base_pretrained_model.trainable = False\n","pt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\n","pt_features = base_pretrained_model(in_lay)\n","from keras.layers import BatchNormalization\n","bn_features = BatchNormalization()(pt_features)\n","\n","\n","attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(Dropout(0.5)(bn_features))\n","attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n","attn_layer = Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n","attn_layer = Conv2D(1, \n","                    kernel_size = (1,1), \n","                    padding = 'valid', \n","                    activation = 'sigmoid')(attn_layer)\n","\n","up_c2_w = np.ones((1, 1, 1, pt_depth))\n","up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n","               activation = 'linear', use_bias = False, weights = [up_c2_w])\n","up_c2.trainable = False\n","attn_layer = up_c2(attn_layer)\n","\n","mask_features = multiply([attn_layer, bn_features])\n","gap_features = GlobalAveragePooling2D()(mask_features)\n","gap_mask = GlobalAveragePooling2D()(attn_layer)\n","\n","gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n","gap_dr = Dropout(0.25)(gap)\n","dr_steps = Dropout(0.25)(Dense(128, activation = 'relu')(gap_dr))\n","out_layer = Dense(t_y.shape[-1], activation = 'softmax')(dr_steps)\n","retina_model = Model(inputs = [in_lay], outputs = [out_layer])\n","from keras.metrics import top_k_categorical_accuracy\n","def top_2_accuracy(in_gt, in_pred):\n","    return top_k_categorical_accuracy(in_gt, in_pred, k=2)\n","\n","retina_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n","                           metrics = ['categorical_accuracy', top_2_accuracy])\n","retina_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"48b9764e16fb5af52aed35c82bae6299e67d5bc7","_cell_guid":"17803ae1-bed8-41a4-9a2c-e66287a24830","trusted":true,"id":"JMVv-2xFBkzt"},"source":["from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n","weight_path=\"{}_weights.best.hdf5\".format('retina')\n","\n","checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n","                             save_best_only=True, mode='min', save_weights_only = True)\n","\n","reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n","early = EarlyStopping(monitor=\"val_loss\", \n","                      mode=\"min\", \n","                      patience=6)\n","callbacks_list = [checkpoint, early, reduceLROnPlat]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"78dfa383c51777377c1f81e42017cbcca5f5736f","_cell_guid":"84f7cdec-ca00-460c-9991-55b1f7f02f20","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true,"id":"DFAbShdRBkzt"},"source":["!rm -rf ~/.keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"b2148479bfe41c5d9fd0faece4c75adea509dabe","_cell_guid":"58a75586-442b-4804-84a6-d63d5a42ea14","trusted":true,"id":"v_LEXLV-Bkzt"},"source":["retina_model.fit_generator(train_gen, \n","                           steps_per_epoch = train_df.shape[0]//batch_size,\n","                           validation_data = valid_gen, \n","                           validation_steps = valid_df.shape[0]//batch_size,\n","                              epochs = 25, \n","                              callbacks = callbacks_list,\n","                             workers = 0, # tf-generators are not thread-safe\n","                             use_multiprocessing=False, \n","                             max_queue_size = 0\n","                            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"3a90f05dd206cd76c72d8c6278ebb93da41ee45f","_cell_guid":"4d0c45b0-bb23-48d2-83eb-bc3990043e26","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true,"id":"fnybzq9JBkzu"},"source":["# load the best version of the model\n","retina_model.load_weights(weight_path)\n","retina_model.save('full_retina_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"2b74f4ab850c6e82549d732b6f0524724b95b53c","_cell_guid":"f37dd4d8-ecd6-487a-90d8-74fe14a9a318","trusted":true,"id":"fV1rh2ZaBkzu"},"source":["from tqdm import tqdm_notebook\n","\n","\n","valid_gen = flow_from_dataframe(valid_idg, valid_df, \n","                             path_col = 'path',\n","                            y_col = 'level_cat') \n","vbatch_count = (valid_df.shape[0]//batch_size-1)\n","out_size = vbatch_count*batch_size\n","test_X = np.zeros((out_size,)+t_x.shape[1:], dtype = np.float32)\n","test_Y = np.zeros((out_size,)+t_y.shape[1:], dtype = np.float32)\n","for i, (c_x, c_y) in zip(tqdm_notebook(range(vbatch_count)), \n","                         valid_gen):\n","    j = i*batch_size\n","    test_X[j:(j+c_x.shape[0])] = c_x\n","    test_Y[j:(j+c_x.shape[0])] = c_y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"cca170eb40bc591f89748ede8aa35de4308faaaf","_cell_guid":"11f33f0a-61eb-488a-b7ea-4bc9d15ba8f9","id":"m_85ksjoBkzu"},"source":["## 5-1. Attention 확인"]},{"cell_type":"code","metadata":{"_uuid":"ad5b085d351e79b950bf0c2ddc476799d5b0692f","_cell_guid":"e41a063f-35c9-410f-be63-f66b63ff9683","trusted":true,"id":"R99YGuLuBkzu"},"source":["# get the attention layer since it is the only one with a single output dim\n","for attn_layer in retina_model.layers:\n","    c_shape = attn_layer.get_output_shape_at(0)\n","    if len(c_shape)==4:\n","        if c_shape[-1]==1:\n","            print(attn_layer)\n","            break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"00850972ae4298f49ed1838b3fc49c2d8fb07547","_cell_guid":"340eef36-f5b2-4b15-a59f-440061a427eb","trusted":true,"id":"h4bFdmYYBkzv"},"source":["import keras.backend as K\n","rand_idx = np.random.choice(range(len(test_X)), size = 6)\n","attn_func = K.function(inputs = [retina_model.get_input_at(0), K.learning_phase()],\n","           outputs = [attn_layer.get_output_at(0)]\n","          )\n","fig, m_axs = plt.subplots(len(rand_idx), 2, figsize = (8, 4*len(rand_idx)))\n","[c_ax.axis('off') for c_ax in m_axs.flatten()]\n","for c_idx, (img_ax, attn_ax) in zip(rand_idx, m_axs):\n","    cur_img = test_X[c_idx:(c_idx+1)]\n","    attn_img = attn_func([cur_img, 0])[0]\n","    img_ax.imshow(np.clip(cur_img[0,:,:,:]*127+127, 0, 255).astype(np.uint8))\n","    attn_ax.imshow(attn_img[0, :, :, 0]/attn_img[0, :, :, 0].max(), cmap = 'viridis', \n","                   vmin = 0, vmax = 1, \n","                   interpolation = 'lanczos')\n","    real_cat = np.argmax(test_Y[c_idx, :])\n","    img_ax.set_title('Eye Image\\nCat:%2d' % (real_cat))\n","    pred_cat = retina_model.predict(cur_img)\n","    attn_ax.set_title('Attention Map\\nPred:%2.2f%%' % (100*pred_cat[0,real_cat]))\n","fig.savefig('attention_map.png', dpi = 300)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"244bac80d1ea2074e47932e367996e32cbab6a3d","_cell_guid":"24796de7-b1e9-4b3b-bcc6-d997aa3e6d16","id":"FAGQgeVjBkzv"},"source":["## 6. 결과 분석"]},{"cell_type":"code","metadata":{"_uuid":"b421b6183b1919a7414482f0b1ac611079e45174","_cell_guid":"d0edaf00-4b7c-4f65-af0b-e5a03b9b8428","trusted":true,"id":"K3C0XoxwBkzv"},"source":["from sklearn.metrics import accuracy_score, classification_report\n","pred_Y = retina_model.predict(test_X, batch_size = 32, verbose = True)\n","pred_Y_cat = np.argmax(pred_Y, -1)\n","test_Y_cat = np.argmax(test_Y, -1)\n","print('Accuracy on Test Data: %2.2f%%' % (accuracy_score(test_Y_cat, pred_Y_cat)))\n","print(classification_report(test_Y_cat, pred_Y_cat))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"10162e055ca7cd52878a289bab377231787ab732","_cell_guid":"15189df2-3fed-495e-9661-97bb2b712dfd","trusted":true,"id":"7dsGCPH6Bkzv"},"source":["import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","sns.heatmap(confusion_matrix(test_Y_cat, pred_Y_cat), \n","            annot=True, fmt=\"d\", cbar = False, cmap = plt.cm.Blues, vmax = test_X.shape[0]//16)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"12dfe39ea80194062068589699953c6645e285d6","_cell_guid":"70827da6-bf91-4b65-80e9-bf1e6b885db3","id":"vY3mEzFABkzw"},"source":["## 6-1. ROC Curve"]},{"cell_type":"code","metadata":{"_uuid":"2b2aaee6c83043f721b0c9ed5bc4229eb7165200","_cell_guid":"829475ab-7db2-4421-b9ad-1a51971fd459","trusted":true,"id":"jagan1QNBkzw"},"source":["\n","from sklearn.metrics import roc_curve, roc_auc_score\n","sick_vec = test_Y_cat>0\n","sick_score = np.sum(pred_Y[:,1:],1)\n","fpr, tpr, _ = roc_curve(sick_vec, sick_score)\n","fig, ax1 = plt.subplots(1,1, figsize = (6, 6), dpi = 150)\n","ax1.plot(fpr, tpr, 'b.-', label = 'Model Prediction (AUC: %2.2f)' % roc_auc_score(sick_vec, sick_score))\n","ax1.plot(fpr, fpr, 'g-', label = 'Random Guessing')\n","ax1.legend()\n","ax1.set_xlabel('False Positive Rate')\n","ax1.set_ylabel('True Positive Rate');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"ba87d0e7c3a77181487b99ca64d13de2aa8a21ee","_cell_guid":"c34f049f-b032-45bf-9d5e-a756ecc46a82","trusted":true,"id":"qF1RMtuNBkzw"},"source":["fig, m_axs = plt.subplots(2, 4, figsize = (32, 20))\n","for (idx, c_ax) in enumerate(m_axs.flatten()):\n","    c_ax.imshow(np.clip(test_X[idx]*127+127,0 , 255).astype(np.uint8), cmap = 'bone')\n","    c_ax.set_title('Actual Severity: {}\\n{}'.format(test_Y_cat[idx], \n","                                                           '\\n'.join(['Predicted %02d (%04.1f%%): %s' % (k, 100*v, '*'*int(10*v)) for k, v in sorted(enumerate(pred_Y[idx]), key = lambda x: -1*x[1])])), loc='left')\n","    c_ax.axis('off')\n","fig.savefig('trained_img_predictions.png', dpi = 300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"eb6752295030ba512263433f8383711e4ca1c14c","_cell_guid":"f2e189dc-f80a-4b16-bb1d-5c05a155a80b","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true,"id":"eb1_HUEVBkzw"},"source":[""],"execution_count":null,"outputs":[]}]}