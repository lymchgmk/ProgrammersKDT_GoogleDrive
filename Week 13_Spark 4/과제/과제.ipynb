{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"과제.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1dCDxhXaMmJ-HjBDhYaOwJAkmG9WXWWaO","authorship_tag":"ABX9TyP+F/whP0huRexUFS92Dwab"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"id":"_EId_XmL2BJr","executionInfo":{"status":"error","timestamp":1628013309963,"user_tz":-540,"elapsed":264,"user":{"displayName":"임창묵","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhscNqM8IitFmuefFNWTyWmxB61RlZpPW5EKPrq=s64","userId":"13261157101031740618"}},"outputId":"ae2e15f9-4e98-4287-ff6f-332d2dea5599"},"source":["f = open('/content/drive/MyDrive/Programmers KDT 과정/Week 14_NLP 1/과제/shakespeare.txt', 'r')\n","lines = f.readlines()\n","lines"],"execution_count":8,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-371d5ca863f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./shakespeare.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './shakespeare.txt'"]}]},{"cell_type":"markdown","metadata":{"id":"4Xwrvs8H1ysc"},"source":["# 1. Negative sampling을 사용해서 임베딩을 학습합니다. 단, 단어 단위가 아닌 subword에 대한 임베딩을 학습하세요. 일반적인 구현은 다음을 참조하세요. https://www.tensorflow.org/tutorials/text/word2vec"]},{"cell_type":"code","metadata":{"id":"4vmWeIG01u8K"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MSnAHWPn11CK"},"source":["# 2. Negative sampling을 사용하지 말고, skip-gram을 Cross entropy loss(https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)를 사용하여 multi-class classification문제로 풀어보세요. 필요하다면(모델학습이 매우 느린 경우에) SentencePiece에서 모델을 학습시 vocabulary size를 제한할 수 있습니다."]},{"cell_type":"code","metadata":{"id":"cEAxwMYa14Pr"},"source":[""],"execution_count":null,"outputs":[]}]}